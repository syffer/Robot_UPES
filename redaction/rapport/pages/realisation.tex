\chapter{Realised work}


This chapter describes how each part of the project hab been realised. It will present the first module with the image modelisation, the transformations implementation, and the \gls{GUI} realisation. Then the second module will be explained with the chain code, the database creation, and the tagging algorithm. This project has been implemented in java.


\section{First module : image processing operations}


\subsection{Images}


The \vref{fig:diagram:class:image} shows the class diagram that was used. The visitor pattern was also used in order to facilitate the implementation of new processes (or algorithms) on a data structure without changing it. It does so by separating the algorithm and the data structure from each other  \cite{bib:pattern:Visitor}. A new process (or algorithm) could be added by extending the VisitorImage class. One class per type of images has been used : RGBImage, GreyImage and MonoImage. 
% and implementing the apply method for each image subclasses 

\begin{figure}[H]
	\centering 
	\includegraphics[width=0.75\textwidth]{images/diagrams/class_diagram_image}
	\caption{Image package class diagram}
	\label{fig:diagram:class:image}
\end{figure}



\subsection{Modelisation of the transformations}


The \vref{fig:diagram:class:transform} shows the class diagram of the transform package. In order to make the reading easier, this diagram has been simplified (e.g. "visit()" methods of concrete classes have been removed), and the Image and VisitorImage classes hab been added (which do not belong to the transform package). 

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/diagrams/class_diagram_transform}
	\caption{Transform package class diagram}
	\label{fig:diagram:class:transform}
\end{figure}


The transform package regroups every operations that create a new image from another. As a transformation can be different depending on the type of the image (e.g. RGB, grey, etc.), the visitor pattern is used. The abstract class Transformation represents a transformation applyied on an image, and returning a new image. It extends from the VisitorImage and provide a way to get back the new image from the visitor pattern. All of the transformations must heritate from the Transformation class. Moreover, differents subpackages are used in order to seperate transformations by type : 
\begin{description}
	\item[morphology] contains the operations that changes the shape of the images (e.g. grey scale, erosion)
	\item[filter] groups the transformations that uses filters, whether it is an edge detection filter or a noise removal filter (e.g. the median filter, the canny filter)
	\item[cellularAutomata] huddles the operations using cellular automata shown in the paper \cite{bib:filter:CA}. For the moment, only the second version of the presented algorithms is implemented
	\item[symbolic] gathers the algorithms invloving symbolic computing
\end{description}






\subsection{GUI}

A \gls{GUI} has been developped during the project in order to see the results of each transformations. The \gls{MVC} pattern is used to create the \gls{GUI}, it is an architectural pattern that separates an application into three parts : the model that contains the data and the logistics of the application, the view that gives a presentation of the model, and the controller who accepts user inputs and links the two previous parts. An example of the \gls{GUI} is shown with the \vref{fig:gui:application}.


\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{images/application/application}
	\caption{Application GUI example}
	\label{fig:gui:application}
\end{figure}


The simplified class diagram of the gui package is shown on the \vref{fig:diagram:class:gui}, some methods aren't displayed and other classes that doesn't belong to the gui package are in order to make the diagram clearer. Each part of the \gls{MVC} pattern is grouped in its own subpackage (the view package regroups the classes belonging to the view part, etc.). 

~~

The View class represents the interface visible by users, and has multiple attributes, like buttons, allowing users to interract with the application. The InternalView represents an internal window in the \gls{GUI}, that can be moved by users. Each InternalView is associated with one InternalModel, and one InternalController. The ImageView represents an image, loaded or transformed, that is displayed in an internal window in the \gls{GUI}. Users are able to transform an image by selecting the appropriate internal window (ImageView), and clicking on the button associated to the desired transformation. 


The GeneralController creates and links the view and the model parts. His other job is to get back the user interractions with the application. To do that, classes extending the AbstractAction are created, and affected on the view buttons. The ActionTransformation class represents an action that will apply a transformation on the selected image, using a Transformation subclass given during the initialization. Others actions exists, like ActionSave, and ActionClose. When the user clic on a button, the associated action is applyied (e.g. a transformation, a loading, etc.), and a subclass of the InternalController is created, which will create the appropriate InternalModel and InternalView subclasses.

~~

Images are stored in the ImageModel classes. The GeneralModel keeps in memory which of the opened image model is selected by the user. Moreover, the GeneralModel is made Observable in order to notify others classes when the user changes his selection. The actions classes implement the Observer interface, and listen to the GeneralModel. When the GeneralModel notifies its observers, the actions classes update themselves, and activate depending on the new selection of the user. 



\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/diagrams/class_diagram_gui}
	\caption{GUI package class diagram}
	\label{fig:diagram:class:gui}
\end{figure}







\section{Second module : object recognition on 2D images}



\subsection{Feature Extraction with the Freeman Chain Code}

The \vref{fig:diagram:class:objects} illustrates the class dagram of the object package. This packages gathers every classes related to the object recognition, that is the chain code, the features, and the objects (tagged or not). A chain code is represented by the ChainCode class, and stores the codes in a list. The SomeObject abstract class represents an object that can be tagged, and posessing some features. The PositionnedObject corresponds to an object extracted from the image, it has a certain position along with a chain code. The Feature class regroups every features an object can have (e.g. width, compactness, etc.). The AnnotatedObjectBean class and the use of the Comparator's interfaces are explained in the following sections. 


\begin{figure}[H]
	\centering 
	\includegraphics[width=1\textwidth]{images/diagrams/class_diagram_objects}
	\caption{Object package class diagram}
	\label{fig:diagram:class:objects}
\end{figure}








\subsection{The database creation}

The database purposes is to store the known tagged objects. It has been decided that those objects will be stores through their features. The database was created with Oracle Database, which contains only one table named "AnnotatedObjects". This table stores the information regarding the tagged objects, which is an id and the tagged object's features (as shown in the previous section).

~~

The \gls{DAO} pattern is used in order to facilitate the access to the database. The \gls{DAO} pattern allows to differentiate the data to be accessed, and the way they are stored. To do so, it separate the business object (in a bean package), how the data are recovered (in a dao package, i.e. the requests), and the storage system (in a session package). For each table in the database, there must be one bean class to store the values and one dao class to access it, nammed after the table's name. Moreover, as it is not necessary to have more than one instance of each dao classes per sessions, a factory pattern is used to store singleton instances of each created dao per session, and to return it when needed.

%diagramme de classe Ã  expliquer 
~~


The \vref{fig:diagram:class:database} displays the class diagram of the database package. An abstract Session class provides a general way of connecting to a database, regarding of the user's id, the database's url, and the \gls{DBMS} used (e.g. Oracle, MySQL). A general abstract DAO class also provide basis requests to be implemented. When a new table is added into the database, it is necessary to create a new bean class, a new dao class by extending the abstract class "DAO", and add a method in the factory in order to be able to access the new dao class.


\begin{figure}[H]
	\centering 
	\includegraphics[width=1\textwidth]{images/diagrams/class_diagram_database}
	\caption{Database package class diagram}
	\label{fig:diagram:class:database}
\end{figure}


\subsection{The selected dataset}

Multiple dataset can be found on the Internet, for example the \href{http://groups.csail.mit.edu/vision/SUN/}{SUN database}, the \href{http://cbcl.mit.edu/software-datasets/streetscenes/}{StreetScenes framework}, the \href{http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html}{PASCAL Visual Object Classes} and the \href{http://labelme2.csail.mit.edu/Release3.0/browserTools/php/dataset.php}{LabelMe dataset}. For this project, it has been chosen to use the last one, the LabelMe dataset, has it seems to be the most suited to the project needs. LabelMe was created by the MIT Computer Science and Artificial Intelligence Laboratory, it's goal is to provide an online annotation tool in order to build image databases for computer vision research. An application was released on the 10$^{th}$ of December 2012 for iPhone and iPad \cite{bib:labelme}.

~~

LabelMe represents the extracted objects in the form of polygons, stored in a xml file. Each annotated images in associated to one xml file, which contains every annotated objects present in that image. For retrieving the dataset (i.e. the images and their annotations), LabelMe provide a Matlab toolbox, which allow to customize to portion of the database the user want to access (i.e. by giving the folders, tags, etc.). As describes in the LabelMe documentation, it is possible to access the dataset either by downloading all the interested images locally and then explore the dataset, or access the dataset online and exlore it before downloading what is necessary. The first method is advised by LabelMe authors, but seems to bug during the downloading of the dataset. The second method was used, Matlab scrips were written in order to specify the folders to explore, which tagged object to recover (by giving the tag), and the local folder were the annotations should be saved.

~~

In order to save the annotated objects from the xml files to the database, a xml parser is used to travel the xml files and get each annotated object through his tag and the polygon. Then the features of the polygon are extracted with the same methods as the chain code, and are stored into the database by using the dao. As for parsing the xml files, the \acrshort{SAX} is used. 

% crÃ©ation d'un parseur xml pour extraire les donnÃ©es


\subsection{The tagging algorithm}

An object to be tagged is compared to others objects that has already been tagged, which are the tagged objects stored in the database. The object to be tagged will then take the tag of the nearest tagged object. Two objects are compared by comparing their features in a predefined order. If the first features of the objects are equals, the second features will be compared, and so on. Two objects are considered equals only if all of their features are equals (to two decimals). 

~~

In order to implement this algorithm, the Feature class has been made comparable, and a comparator for each attributes (i.e. the features of the object, the width, the compactness, etc.) has been created. A general comparator is used in order to compare the features in a global manner, it is in this general comparator that the features comparing order is defined. 

~~

As the tagged objects list could be large, instead of crossing the list completely, a "quick sort like" research algorithm is used. For this algorithm to work, it is necessary to sort the list of the tagged objects, which is done with the sql request by ordering every fields (i.e. columns of the table) in ascending order. The algorithm define a pivot (i.e. the middle of the list boundaries), and compare the pivot with the object to be tagged. Depending on the result of the comparison, the nearest tagged object is either before the pivot, or after. The list boundaries are then changed, the pivot is updated, and compared again with the object to be tagged until the boundaries are incorrects. This will give an estimation of the nearest tagged object position, the nearest object will be between the tagged objects before and after the pivot. To get the nearest tagged object, no more than three tagged objects are to be compared with the untagged object. The \vref{lst:simplified tagging algorithm} shows the pseudocode of the algorithm.

\begin{listing}
	\begin{minted}[mathescape=true]{java}
public AnnotatedObjectBean getNearestAnnotated(SomeObject object, List<AnnotatedObjectBean> others) {
	pivot, lower @$\leftarrow$@ 0, difference @$\leftarrow$@ 0
	higher @$\leftarrow$@ others.size()
	
	while (lower < higher) { // get an estimation of the nearest object position 
		pivot @$\leftarrow$@ (lower + higher) / 2
		AnnotatedObjectBean other @$\leftarrow$@ others.get(pivot)
		difference @$\leftarrow$@ object.compareTo(other)
		
		if (difference > 0) lower @$\leftarrow$@ pivot + 1  // change the boundaries accordingly 
		else if (difference < 0) higher @$\leftarrow$@ pivot;
		else return other;
	}
	
	difference @$\leftarrow |$@difference@$|$@  // absolute value to get the nearest
	for (i in [pivot-1, pivot+1]) {
		if (i < 0 || i >= others.size()) continue
		
		AnnotatedObjectBean other @$\leftarrow$@ others.get(i)
		diff = @$|$@object.compareTo(other)@$|$@
		
		if (diff < difference) { // a nearest object has been found
			difference @$\leftarrow$@ diff
			pivot @$\leftarrow$@ i
		}
	}
	
	return others.get(pivot)	
}
	\end{minted}
	\caption{Simplified tagging algorithm pseudocode}
	\label{lst:simplified tagging algorithm}
\end{listing}



% comparer chaque champs un par un 
% donner l'ordre des champs 

% ordre de prÃ©fÃ©rence (d'abord lui, puis lui, puis lui...)
% utilisation de comparator pour pouvoir comparer les obj via leurs features
% - un comparator par featur 
% - un comparator global utilisant les autres comparator dans un ordre spÃ©cific

% rÃ©cupÃ©ration des obj annotÃ© de la db via dao
% trouver l'obj annotÃ© le plus proche de l'objet Ã  annoter
% - ordoner les obj annotÃ© du plus petit au plus grand via l'instruction 'order by' de la requete sql
% - utilisation du quicksort pour obtenir un interval dans lequel l'obj le plus proche se trouve
% - comparer les trois derniers objets (les obj annotÃ© de l'interval), lequel est vraiment le plus proche de l'obj Ã  annoter




\subsection{Classification with K-means algorithm}

Some definitions related to \gls{data analysis} and \gls{classification} area can be found in the glossary section. The K-means \gls{algorithm} is a non-hierarchical \gls{classification} \gls{algorithm} (i.e. during the \gls{classification}, no hierarchy between \glspl{individual} is used). For this project, it has been decided to implement the K-means \gls{algorithm} instead of the \gls{EM} because K-means shows better results. 

~~ 

The principle of K-means is the following (\cite{bib:clustering:AnalyseDesDonnees}). First, $n$ \glspl{individual} are chosen to be the initial gravity center of each \glspl{cluster} (where $n$ is the number of clusters). Then, each individual is affected (added) to the nearest \gls{cluster} by comparing the \gls{individual} and the \gls{cluster}'s gravity center, the \gls{cluster}'s gravity center is then  updated accordingly. The second step is repeated until the \gls{algorithm} converge (i.e. the iteration when no \gls{individual} changes \gls{cluster}). The gravity center of each \gls{cluster} is updated each time an \gls{individual} is added or removed of the \gls{cluster}.

~~

The \vref{fig:diagram:class:kmeans} shows the class diagram used to implement the K-means \gls{algorithm}. The class Individual contains a certain number of variables. The class Cluster gathers a list of individuals with the same number of variables. After performing the classification, the K-means \gls{algorithm} will return a list of $n$ Clusters. Moreover, an array "classes" in the KMeans class stores the \gls{cluster}'s number (between $[0;n[$) affected to each \gls{individual}.

~~

The K-means \gls{algorithm} has some defaults : 
\begin{itemize}
	\item it is necessary to know the number of clusters before the execution of the \gls{algorithm}
	\item K-means can return different results for the same input (as it chooses random \glspl{individual} for initializing the \glspl{cluster})
\end{itemize}


\begin{figure}[H]
	\centering 
	\includegraphics[width=1\textwidth]{images/diagrams/class_diagram_kmeans}
	\caption{K-means class diagram}
	\label{fig:diagram:class:kmeans}
\end{figure}



