\chapter{Realised work}

general procedure 

<diagram flowchart>


\section{First module : 3D map generation}


\subsection{Procedure}

=> est une introduction 

<diagram flowchart>


\subsection{Images}

An image is represented by a width, an heigth, colors (stocked in pixel values, i.e. positive integers), and a color model. A color model describes how the colors are represented, that is how many values should be used in order to represent a color \cite{bib:image:ColorModel}. Multiples color models exist, for example : 
\begin{itemize}
	\item RGB, where a color is represented by the three primary colors (red, green and blue)
	\item CMY, where a color is represented by a combinaison of cyan, magenta and yellow colors
\end{itemize}

% cours de jpeg 

~~

We focused of three types of images : RGB, Grey and Monochromatic images. A grey image is an image where the pixels are represented in a grey scale by only one value, the grey value (which can have any integer values between 0 and 255 included). In other words, there is 256 possible colors in a grey images, which are all shade of grey. In a monochromatic image, only two colors are represented, black (with the value 0) and white (with the value 255).

~~

Once we loaded an image in the RGB color model, we converted it into a grey image in order to reduce the number of information present in the image and lower the complexity. The monochromatic image were used in order to determinate edges and extract the objects in the image.

~~

The grey value of a RGB pixel is determined by doing the average of the red, green and blue values. As for the monochromatic image, we first have to define a 
threshold value, then a pixel will be represented in black if it's grey value is strictly under the threshold, and in white otherwise (the equation \ref{eq:threshold} show it in a more mathematical point of view) \cite{bib:image:Threshold}.

\begin{equation} \label{eq:threshold}
mono_{threshold}(greyValue) = 
\begin{cases}
	BLACK & \text{if } greyValue < threshold\\
	WHITE & \text{otherwise} \\ 
\end{cases}
\end{equation}


We choosed to represent the grey and monochromatic image by a one matrix. 


.... 



<changer RGB image> 

\begin{figure}
	\centering 
	\includegraphics[width=0.75\textwidth]{images/diagrams/class_diagram_image}
	\caption{Image package class diagram}
	\label{fig:diagram:class:image}
\end{figure}


The visitor pattern was also used in order to facilitate the implementation of new processes (or algorithms) on a data structure without changing it. It does so by separating the algorithm and the data structure from each other. A new process (or algorithm) could be added by extending the VisitorImage class and implementing the apply method for each image subclasses \cite{bib:pattern:Visitor}. 

% + cours 
% + book

~~

The axis we used to represent an image are illustrated in the \vref{fig:axis representation}. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/axis/axis_representation}
	\caption{Axis representation}
	\label{fig:axis representation}
\end{figure}




\subsection{Rajan Tranform and Set Theoretic Rajan Transform}

We planned to use the Set Theoretic Rajan Transform (STRT) in our process but it wasn't possible due to some issues and imprecisions in the documentation concerning the STRT$^{-1}$. We wanted to use this transformation on an image mainly because it has noise removal properties.

~~

The Set Theoretic Rajan Transform (STRT) correspond to the Rajan Transform (RT) applied on the sets domain. First I will present the forward rajan transform, and then it's application in the sets domain. Every equations, figures and explainations have been inspired by the following reference  \cite{bib:symbolic:RajanTransform}.

~~

The rajan transform take a sequence of $2^{i}$ numbers (the number of elements of the sequence have to be a power of 2), transform it, and return another sequence of $2^{i}$ numbers. We will call $x(k)$ the input sequence, $X(k)$ the output sequence of the tranform, and $N$ the number of element in the sequence \cite{bib:symbolic:RajanTransform}. 

~~

We can define the sequences $x(k)$, $g(k)$ and $h(k)$ as following : 
\begin{align}
x &= x(0), x(2), \cdots, x(k-1) & \text{where k is a power of 2} \\
g(k) &= x(k) + x(k + \frac{N}{2}) & \text{with } 0 \leq k \leq N / 2 \\
h(k) &= | x(k) - x(k - \frac{N}{2}) | & \text{with } N / 2 \leq k \leq N
\end{align}

~~

In other words, the sequence $x$ will be divided in two. Then we will sum the two first value of the subsequences (which will give us the results of $g(1)$), then the two seconds values (which is $g(2)$), then the thirds values, and so on until all the elements were processed. Then we will do the same operations but with a substraction instead (to get the sequence $h$).

~~

This processus will then have to be repeated on the subsequences $g$ and $h$ separately. We can see here the recursive character of this transformation. The figure \ref{fig:diagram:flowchart:rajan} illustrate the rajan tranformation in a more procedural way, with a sequence of 4 elements. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{images/diagrams/flowchart_rajan_transform}
	\caption{The forward Rajan Tranfform (RT)}
	\label{fig:diagram:flowchart:rajan}	
\end{figure}


The Set Theoretic Rajan Transform is the application of the Rajan Transform in the sets domain, and so instead of tranforming a sequence of numbers, it will tranform a sequence of sets, and return another sequence of sets (e.g. $x(1)$ and $X(1)$ wouldn't be numbers, but two sets). In the set domain, the addition correspond to the union, and the substraction to the difference of two sets.





\subsection{Filters}

\subsection{Cellular Automata algorithm}

\subsection{Morphology}




\subsection{Feature Extraction and Freeman Chain Code}


We defined a feature as something in the image that have a certain shape and edges. 


\subsubsection{Freeman Chain Code and it's extraction}

The Freeman Chain Code is a coding edge algorithm. It allows to represent a shape given in a bimary image (monochromatic) by it's edge. It's first purpose is to compress the data, as we pass from a binary image to a chain (or list) of code representing the shape of the edge \cite{bib:chain:ParametreGeometriqueChaineFreeman}. In other words, it's a data structure for representing the boundary of a feature \cite{bib:chain:DigitalImageProcessing}.

~~

In order to extract the chain code of a shape in a binary image, it is necessary to start from on of the end edges. For this project, it has been decided to start from the most left upper edge point of the shape, i.e. there isn't any other edge point that is at the left of the starting point. In order to have this starting point, the image has been travelled from up to down, and then from left to right.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{images/chain_code/course_order}
	\caption{Course order of the binary image \cite{bib:chain:ParametreGeometriqueChaineFreeman}}
	\label{fig:chain code:course order}	
\end{figure}

Once the starting point is determined, it is then possible to follow the boundary in the anticlockwise direction. Then, depending on the direction, a code is generated according to the following direction scheme illustrated with the \vref{fig:chain code:direction scheme star}, where the central point of the star represent the current boundary pixel. For example, if the next boundary pixel is on the top of the current one, the code 2 will be generated. 


\begin{figure}[H]
	\centering
	\includegraphics[width=0.2\textwidth]{images/chain_code/direction_scheme_star}
	\caption{Direction scheme star \cite{bib:chain:ParametreGeometriqueChaineFreeman}}
	\label{fig:chain code:direction scheme star}	
\end{figure}


Moreover, in order to support more complexe shapes, the pixel neibors of the current boundary pixel are travelled depending on the last generated code (i.e. on the last direction). In other words, the stating point of the star is changed depending on the last direction. The first pixel neibor that should be visited must be the previous pixel boundary (i.e. the origin of the direction used to get on the current pixel). 


\subsubsection{Feature Extraction}


As we said before, in order to detect the different edges in th eimage, we used a filter edge detection, like sobel or canny. 

In order to extracts the differents features in the image, 





\subsection{Classification}

kmean > EM because show better results 
but - quand mÃªme (choisir nb classes + random with gravity center initialisation)


\subsection{GUI}

A Graphic User Interface has been developped during the project in order to see the results.... 


\section{Second module}

